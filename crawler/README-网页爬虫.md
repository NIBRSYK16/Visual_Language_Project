# 网页爬虫数据增强工具使用说明

## 概述

这个工具从论文链接（DOI链接）直接爬取论文的摘要、关键词、作者机构等信息，比使用API更直接可靠。

## 安装依赖

```bash
cd crawler
pip install -r requirements.txt
```

新增依赖：
- `beautifulsoup4` - HTML解析
- `html5lib` - HTML解析器（可选，但推荐）
- `lxml` - 快速XML/HTML解析器（可选）

## 使用方法

### 基本使用

```bash
python enhance_with_scraper.py
```

运行时会提示：
1. 选择处理范围（全部/部分/前N篇）
2. 确认开始处理

### 处理选项

1. **处理所有论文**：处理所有3641篇论文（耗时较长，约30小时）
2. **处理部分论文**：指定索引范围（如 0-1000）
3. **处理前N篇**：测试用，处理前N篇论文（推荐先测试100篇）

## 支持的出版商

- **ACM Digital Library**：`10.1145` 开头的DOI
- **IEEE Xplore**：`10.1109` 开头的DOI
- **其他**：通用提取（可能效果有限）

## 可获取的信息

- ✅ **摘要（Abstract）**：从论文页面提取
- ✅ **关键词（Keywords）**：从论文页面提取
- ✅ **作者机构（Affiliations）**：从论文页面提取
- ✅ **国家信息（Country）**：从机构信息推断

## 注意事项

1. **请求频率**：
   - 每次请求后延迟0.5秒
   - 每10次请求后延迟1.5秒
   - 避免请求过快被服务器拒绝

2. **网络要求**：
   - 需要稳定的网络连接
   - 某些论文可能需要VPN（如果在中国大陆）

3. **成功率**：
   - 不是所有论文都能成功爬取
   - 某些论文页面结构可能不同
   - 部分论文可能需要登录才能访问

4. **运行时间**：
   - 每篇论文约0.5-1秒
   - 100篇论文约1-2分钟
   - 3641篇论文约30-60分钟（实际可能更长）

5. **错误处理**：
   - 如果某篇论文爬取失败，会保留原始数据
   - 不会因为单篇论文失败而中断整个流程

## 测试

运行测试脚本验证爬虫是否正常工作：

```bash
python web_scraper.py
```

这会测试几个示例URL，查看是否能成功提取信息。

## 故障排查

### 问题：无法提取摘要

- 检查网络连接
- 确认论文URL是否可访问
- 某些论文可能需要登录

### 问题：提取的摘要为空

- 论文页面结构可能不同
- 可以手动检查页面，看看摘要的HTML结构
- 可能需要更新选择器

### 问题：请求被拒绝

- 增加延迟时间
- 检查User-Agent设置
- 可能需要使用代理

## 与其他方法的比较

| 方法 | 优点 | 缺点 |
|------|------|------|
| 网页爬虫 | 数据真实、免费、可获取机构信息 | 需要处理不同页面结构、可能被限制 |
| Semantic Scholar API | 统一接口、数据相对完整 | 可能匹配不到、有API限制 |
| 大模型API | 可以生成摘要、关键词 | 成本高、可能不准确 |

## 建议

1. **先测试**：处理前100篇论文，验证效果
2. **分批处理**：如果数据量大，分批处理避免中断
3. **结合使用**：可以先用爬虫，再用API补充缺失的部分
